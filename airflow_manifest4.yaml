---
# Source: airflow/templates/rbac/airflow-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: airflow
  labels:
    app: airflow
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
---
# Source: airflow/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-postgresql
  labels:
    app: postgresql
    chart: postgresql-8.6.4
    release: "airflow"
    heritage: "Helm"
type: Opaque
data:
  postgresql-password: "YWlyZmxvdw=="
---
# Source: airflow/charts/redis/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-redis
  labels:
    app: redis
    chart: redis-10.5.7
    release: "airflow"
    heritage: "Helm"
type: Opaque
data:
  redis-password: "YWlyZmxvdw=="
---
# Source: airflow/templates/config/secret-config-envs.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-config-envs
  labels:
    app: airflow
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
## we must use `data` rather than `stringData` (see: https://github.com/helm/helm/issues/10010)
data:
  ## ================
  ## Linux Configs
  ## ================
  TZ: "RXRjL1VUQw=="

  ## ================
  ## Database Configs
  ## ================
  ## database host/port
  DATABASE_HOST: "YWlyZmxvdy1wb3N0Z3Jlc3FsLmFpcmZsb3ctbG9ncy5zdmMuY2x1c3Rlci5sb2NhbA=="
  DATABASE_PORT: "NTQzMg=="

  ## database configs
  DATABASE_DB: "YWlyZmxvdw=="

  ## bash command which echos the URL encoded value of $DATABASE_USER
  DATABASE_USER_CMD: "ZWNobyAiJHtEQVRBQkFTRV9VU0VSfSIgfCBweXRob24zIC1jICJpbXBvcnQgdXJsbGliLnBhcnNlOyBlbmNvZGVkX3VzZXIgPSB1cmxsaWIucGFyc2UucXVvdGUoaW5wdXQoKSk7IHByaW50KGVuY29kZWRfdXNlciki"

  ## bash command which echos the URL encoded value of $DATABASE_PASSWORD
  DATABASE_PASSWORD_CMD: "ZWNobyAiJHtEQVRBQkFTRV9QQVNTV09SRH0iIHwgcHl0aG9uMyAtYyAiaW1wb3J0IHVybGxpYi5wYXJzZTsgZW5jb2RlZF9wYXNzID0gdXJsbGliLnBhcnNlLnF1b3RlKGlucHV0KCkpOyBwcmludChlbmNvZGVkX3Bhc3MpIg=="

  ## bash command which echos the DB connection string in SQLAlchemy format
  DATABASE_SQLALCHEMY_CMD: "ZWNobyAtbiAicG9zdGdyZXNxbCtwc3ljb3BnMjovLyQoZXZhbCAkREFUQUJBU0VfVVNFUl9DTUQpOiQoZXZhbCAkREFUQUJBU0VfUEFTU1dPUkRfQ01EKUAke0RBVEFCQVNFX0hPU1R9OiR7REFUQUJBU0VfUE9SVH0vJHtEQVRBQkFTRV9EQn0i"

  ## bash command which echos the DB connection string in Celery result_backend format
  DATABASE_CELERY_CMD: "ZWNobyAtbiAiZGIrcG9zdGdyZXNxbDovLyQoZXZhbCAkREFUQUJBU0VfVVNFUl9DTUQpOiQoZXZhbCAkREFUQUJBU0VfUEFTU1dPUkRfQ01EKUAke0RBVEFCQVNFX0hPU1R9OiR7REFUQUJBU0VfUE9SVH0vJHtEQVRBQkFTRV9EQn0i"

  ## ================
  ## Redis Configs
  ## ================
  ## connection string components
  REDIS_HOST: "YWlyZmxvdy1yZWRpcy1tYXN0ZXIuYWlyZmxvdy1sb2dzLnN2Yy5jbHVzdGVyLmxvY2Fs"
  REDIS_PORT: "NjM3OQ=="
  REDIS_DBNUM: "MQ=="

  ## bash command which echos the URL encoded value of $REDIS_PASSWORD
  ## NOTE: if $REDIS_PASSWORD is non-empty, prints `:${REDIS_PASSWORD}@`, else ``
  REDIS_PASSWORD_CMD: "ZWNobyAiJHtSRURJU19QQVNTV09SRH0iIHwgcHl0aG9uMyAtYyAiaW1wb3J0IHVybGxpYi5wYXJzZTsgZW5jb2RlZF9wYXNzID0gdXJsbGliLnBhcnNlLnF1b3RlKGlucHV0KCkpOyBwcmludChmXCI6e2VuY29kZWRfcGFzc31AXCIpIGlmIGxlbihlbmNvZGVkX3Bhc3MpID4gMCBlbHNlIE5vbmUi"

  ## bash command which echos the Redis connection string
  REDIS_CONNECTION_CMD: "ZWNobyAtbiAicmVkaXM6Ly8kKGV2YWwgJFJFRElTX1BBU1NXT1JEX0NNRCkke1JFRElTX0hPU1R9OiR7UkVESVNfUE9SVH0vJHtSRURJU19EQk5VTX0ke1JFRElTX1BST1BFUlRJRVN9Ig=="

  ## ================
  ## Airflow Configs (General)
  ## ================
  AIRFLOW__CORE__DAGS_FOLDER: "L29wdC9haXJmbG93L2RhZ3M="
  AIRFLOW__CORE__EXECUTOR: "Q2VsZXJ5RXhlY3V0b3I="
  AIRFLOW__CORE__FERNET_KEY: "N1Q1MTJVWFNTbUJPa3BXaW1GSElWYjhqSzZsZm1TQXZ4NG1PNkFyZWhuYz0="
  AIRFLOW__WEBSERVER__SECRET_KEY: "U2hvdWxkTm90QmVJblByb2R1Y3Rpb24="
  AIRFLOW__WEBSERVER__WEB_SERVER_PORT: "ODA4MA=="
  AIRFLOW__CELERY__FLOWER_PORT: "NTU1NQ=="

  ## ================
  ## Airflow Configs (Database)
  ## ================
  AIRFLOW__CORE__SQL_ALCHEMY_CONN_CMD: "YmFzaCAtYyAnZXZhbCAiJERBVEFCQVNFX1NRTEFMQ0hFTVlfQ01EIic="
  ## `core.sql_alchemy_conn` moved to `database.sql_alchemy_conn` in airflow 2.3.0
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN_CMD: "YmFzaCAtYyAnZXZhbCAiJERBVEFCQVNFX1NRTEFMQ0hFTVlfQ01EIic="

  ## ================
  ## Airflow Configs (Triggerer)
  ## ================
  AIRFLOW__TRIGGERER__DEFAULT_CAPACITY: "MTAwMA=="

  ## ================
  ## Airflow Configs (Logging)
  ## ================
  AIRFLOW__LOGGING__BASE_LOG_FOLDER: "L29wdC9haXJmbG93L2xvZ3M="
  AIRFLOW__LOGGING__DAG_PROCESSOR_MANAGER_LOG_LOCATION: "L29wdC9haXJmbG93L2xvZ3MvZGFnX3Byb2Nlc3Nvcl9tYW5hZ2VyL2RhZ19wcm9jZXNzb3JfbWFuYWdlci5sb2c="
  AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY: "L29wdC9haXJmbG93L2xvZ3Mvc2NoZWR1bGVy"

  ## ================
  ## Airflow Configs (Celery)
  ## ================
  AIRFLOW__LOGGING__WORKER_LOG_SERVER_PORT: "ODc5Mw=="
  # `logging.worker_log_server_port` replaced `celery.worker_log_server_port` in airflow 2.2.0
  AIRFLOW__CELERY__WORKER_LOG_SERVER_PORT: "ODc5Mw=="
  AIRFLOW__CELERY__BROKER_URL_CMD: "YmFzaCAtYyAnZXZhbCAiJFJFRElTX0NPTk5FQ1RJT05fQ01EIic="
  AIRFLOW__CELERY__RESULT_BACKEND_CMD: "YmFzaCAtYyAnZXZhbCAiJERBVEFCQVNFX0NFTEVSWV9DTUQiJw=="

  ## ================
  ## Airflow Configs (Kubernetes)
  ## ================

  ## ================
  ## User Configs
  ## ================
---
# Source: airflow/templates/config/secret-webserver-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-webserver-config
  labels:
    app: airflow
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
data:
  webserver_config.py: "ZnJvbSBmbGFza19hcHBidWlsZGVyLnNlY3VyaXR5Lm1hbmFnZXIgaW1wb3J0IEFVVEhfREIKCiMgdXNlIGVtYmVkZGVkIERCIGZvciBhdXRoCkFVVEhfVFlQRSA9IEFVVEhfREIK"
---
# Source: airflow/templates/db-migrations/db-migrations-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-db-migrations
  labels:
    app: airflow
    component: db-migrations
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
data:
  db_migrations.py: "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"
---
# Source: airflow/templates/sync/sync-users-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-sync-users
  labels:
    app: airflow
    component: sync-users
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
data:
  sync_users.py: "
############################
#### BEGIN: GLOBAL CODE ####
############################
####################
## Global Imports ##
####################
import logging
import os
import time
from string import Template
from typing import List, Dict, Optional


####################
## Global Configs ##
####################
# the path which Secret/ConfigMap are mounted to
CONF__TEMPLATES_PATH = "/mnt/templates"

# how frequently to check for Secret/ConfigMap updates
CONF__TEMPLATES_SYNC_INTERVAL = 10

# how frequently to re-sync objects (Connections, Pools, Users, Variables)
CONF__OBJECTS_SYNC_INTERVAL = 60


######################
## Global Functions ##
######################
def string_substitution(raw_string: Optional[str], substitution_map: Dict[str, str]) -> str:
    """
    Apply bash-like substitutions to a raw string.

    Example:
    - string_substitution("Hello!", None) -> "Hello!"
    - string_substitution("Hello ${NAME}!", {"NAME": "Airflow"}) -> "Hello Airflow!"
    """
    if raw_string and len(substitution_map) > 0:
        tpl = Template(raw_string)
        return tpl.safe_substitute(substitution_map)
    else:
        return raw_string


def template_mtime(template_name: str) -> float:
    """
    Return the modification-time of the file storing `template_name`
    """
    file_path = f"{CONF__TEMPLATES_PATH}/{template_name}"
    return os.stat(file_path).st_mtime


def template_value(template_name: str) -> str:
    """
    Return the contents of the file storing `template_name`
    """
    file_path = f"{CONF__TEMPLATES_PATH}/{template_name}"
    with open(file_path, "r") as f:
        return f.read()


def refresh_template_cache(template_names: List[str],
                           template_mtime_cache: Dict[str, float],
                           template_value_cache: Dict[str, str]) -> List[str]:
    """
    Refresh the provided dictionary caches of template values & mtimes.

    :param template_names: the names of all templates to refresh
    :param template_mtime_cache: the dictionary cache of template file modification-times
    :param template_value_cache: the dictionary cache of template values
    :return: the names of templates which changed
    """
    changed_templates = []
    for template_name in template_names:
        old_mtime = template_mtime_cache.get(template_name, None)
        new_mtime = template_mtime(template_name)
        # first, check if the files were modified
        if old_mtime != new_mtime:
            old_value = template_value_cache.get(template_name, None)
            new_value = template_value(template_name)
            # second, check if the value actually changed
            if old_value != new_value:
                template_value_cache[template_name] = new_value
                changed_templates += [template_name]
            template_mtime_cache[template_name] = new_mtime
    return changed_templates


def main(sync_forever: bool):
    # initial sync of template cache
    refresh_template_cache(
        template_names=VAR__TEMPLATE_NAMES,
        template_mtime_cache=VAR__TEMPLATE_MTIME_CACHE,
        template_value_cache=VAR__TEMPLATE_VALUE_CACHE
    )

    # initial sync of objects into Airflow DB
    sync_with_airflow()

    if sync_forever:
        # define variables used to track how long since last refresh/sync
        templates_sync_epoch = time.time()
        objects_sync_epoch = time.time()

        # main loop
        while True:
            # monitor for template secret/configmap updates
            if (time.time() - templates_sync_epoch) > CONF__TEMPLATES_SYNC_INTERVAL:
                logging.debug(f"template sync interval reached, re-syncing all templates...")
                changed_templates = refresh_template_cache(
                    template_names=VAR__TEMPLATE_NAMES,
                    template_mtime_cache=VAR__TEMPLATE_MTIME_CACHE,
                    template_value_cache=VAR__TEMPLATE_VALUE_CACHE
                )
                templates_sync_epoch = time.time()
                if changed_templates:
                    logging.info(f"template values have changed: [{','.join(changed_templates)}]")
                    sync_with_airflow()
                    objects_sync_epoch = time.time()

            # monitor for external changes to objects (like from UI)
            if (time.time() - objects_sync_epoch) > CONF__OBJECTS_SYNC_INTERVAL:
                logging.debug(f"sync interval reached, re-syncing all objects...")
                sync_with_airflow()
                objects_sync_epoch = time.time()

            # ensure we dont loop too fast
            time.sleep(0.5)
##########################
#### END: GLOBAL CODE ####
##########################


#############
## Imports ##
#############
import sys
from werkzeug.security import check_password_hash, generate_password_hash
import airflow.www.app as www_app
flask_app = www_app.create_app()
flask_appbuilder = flask_app.appbuilder

# airflow moves the User and Role models around in different versions
try:
  # since 2.7.0
  from airflow.auth.managers.fab.models import User, Role
except ModuleNotFoundError:
  try:
    # from 2.3.0 to 2.6.3
    from airflow.www.fab_security.sqla.models import User, Role
  except ModuleNotFoundError:
    # before 2.3.0
    from flask_appbuilder.security.sqla.models import User, Role


#############
## Classes ##
#############
class UserWrapper(object):
    def __init__(
            self,
            username: str,
            first_name: Optional[str] = None,
            last_name: Optional[str] = None,
            email: Optional[str] = None,
            roles: Optional[List[str]] = None,
            password: Optional[str] = None
    ):
        self.username = username
        self._first_name = first_name
        self._last_name = last_name
        self._email = email
        self.roles = roles
        self._password = password

    @property
    def first_name(self) -> str:
        return string_substitution(self._first_name, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def last_name(self) -> str:
        return string_substitution(self._last_name, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def email(self) -> str:
        return string_substitution(self._email, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def password(self) -> str:
        return string_substitution(self._password, VAR__TEMPLATE_VALUE_CACHE)

    def as_dict(self) -> Dict[str, str]:
        return {
            "username": self.username,
            "first_name": self.first_name,
            "last_name": self.last_name,
            "email": self.email,
            "roles": [find_role(role_name=role_name) for role_name in self.roles],
            "password": self.password
        }


###############
## Variables ##
###############
VAR__TEMPLATE_NAMES = [
]
VAR__TEMPLATE_MTIME_CACHE = {}
VAR__TEMPLATE_VALUE_CACHE = {}
VAR__USER_WRAPPERS = {
  "admin": UserWrapper(
    username="admin",
    first_name="admin",
    last_name="admin",
    email="admin@example.com",
    roles=[        "Admin",
    ],
    password="admin",
  ),
}


###############
## Functions ##
###############
def find_role(role_name: str) -> Role:
    """
    Get the FAB Role model associated with a `role_name`.
    """
    found_role = flask_appbuilder.sm.find_role(role_name)
    if found_role:
        return found_role
    else:
        valid_roles = flask_appbuilder.sm.get_all_roles()
        logging.error(f"Failed to find role=`{role_name}`, valid roles are: {valid_roles}")
        sys.exit(1)


def compare_role_lists(role_list_1: List[Role], role_list_2: List[Role]) -> bool:
    """
    Check if two lists of FAB Roles contain the same roles (ignores duplicates and order).
    """
    name_set_1 = set(role.name for role in role_list_1)
    name_set_2 = set(role.name for role in role_list_2)
    return name_set_1 == name_set_2



def compare_users(user_dict: Dict, user_model: User) -> bool:
    """
    Check if user info (stored in dict) is identical to a FAB User model.
    """
    return (
            user_dict["username"] == user_model.username
            and user_dict["first_name"] == user_model.first_name
            and user_dict["last_name"] == user_model.last_name
            and user_dict["email"] == user_model.email
            and compare_role_lists(user_dict["roles"], user_model.roles)
            and check_password_hash(pwhash=user_model.password, password=user_dict["password"])
    )


def sync_user(user_wrapper: UserWrapper) -> None:
    """
    Sync the User defined by a provided UserWrapper into the FAB DB.
    """
    username = user_wrapper.username
    u_new = user_wrapper.as_dict()
    u_old = flask_appbuilder.sm.find_user(username=username)

    if not u_old:
        logging.info(f"User=`{username}` is missing, adding...")
        created_user = flask_appbuilder.sm.add_user(
            username=u_new["username"],
            first_name=u_new["first_name"],
            last_name=u_new["last_name"],
            email=u_new["email"],
            # in old versions of flask_appbuilder `add_user(role=` can only add exactly one role
            # (unchecked 0 index is safe because we require at least one role using helm values validation)
            role=u_new["roles"][0],
            password=u_new["password"]
        )
        if created_user:
            # add the full list of roles (we only added the first one above)
            created_user.roles = u_new["roles"]
            logging.info(f"User=`{username}` was successfully added.")
        else:
            logging.error(f"Failed to add User=`{username}`")
            sys.exit(1)
    else:
        if compare_users(u_new, u_old):
            pass
        else:
            logging.info(f"User=`{username}` exists but has changed, updating...")
            u_old.first_name = u_new["first_name"]
            u_old.last_name = u_new["last_name"]
            u_old.email = u_new["email"]
            u_old.roles = u_new["roles"]
            u_old.password = generate_password_hash(u_new["password"])
            # strange check for False is because update_user() returns None for success
            # but in future might return the User model
            if not (flask_appbuilder.sm.update_user(u_old) is False):
                logging.info(f"User=`{username}` was successfully updated.")
            else:
                logging.error(f"Failed to update User=`{username}`")
                sys.exit(1)


def sync_all_users(user_wrappers: Dict[str, UserWrapper]) -> None:
    """
    Sync all users in provided `user_wrappers`.
    """
    logging.info("BEGIN: airflow users sync")
    for user_wrapper in user_wrappers.values():
        sync_user(user_wrapper)
    logging.info("END: airflow users sync")

    # ensures than any SQLAlchemy sessions are closed (so we don't hold a connection to the database)
    flask_app.do_teardown_appcontext()


def sync_with_airflow() -> None:
    """
    Preform a sync of all objects with airflow (note, `sync_with_airflow()` is called in `main()` template).
    """
    sync_all_users(user_wrappers=VAR__USER_WRAPPERS)


##############
## Run Main ##
##############
main(sync_forever=True)"
---
# Source: airflow/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-redis
  labels:
    app: redis
    chart: redis-10.5.7
    heritage: Helm
    release: airflow
data:
  redis.conf: |-
    # User-supplied configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
  master.conf: |-
    dir /data
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
  replica.conf: |-
    dir /data
    slave-read-only yes
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
---
# Source: airflow/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-redis-health
  labels:
    app: redis
    chart: redis-10.5.7
    heritage: Helm
    release: airflow
data:
  ping_readiness_local.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_PASSWORD --no-auth-warning \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_PASSWORD --no-auth-warning \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_MASTER_PASSWORD --no-auth-warning \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_MASTER_PASSWORD --no-auth-warning \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: airflow/templates/rbac/airflow-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: airflow
  labels:
    app: airflow
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
rules:
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - "get"
  - "list"
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - "create"
  - "get"
  - "delete"
  - "list"
  - "patch"
  - "watch"
- apiGroups:
  - ""
  resources:
  - "pods/log"
  verbs:
  - "get"
  - "list"
- apiGroups:
  - ""
  resources:
  - "pods/exec"
  verbs:
  - "create"
  - "get"
---
# Source: airflow/templates/rbac/airflow-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: airflow
  labels:
    app: airflow
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: airflow
subjects:
- kind: ServiceAccount
  name: airflow
  namespace: airflow-logs
---
# Source: airflow/charts/postgresql/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-postgresql-headless
  labels:
    app: postgresql
    chart: postgresql-8.6.4
    release: "airflow"
    heritage: "Helm"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: postgresql
    release: "airflow"
---
# Source: airflow/charts/postgresql/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-postgresql
  labels:
    app: postgresql
    chart: postgresql-8.6.4
    release: "airflow"
    heritage: "Helm"
spec:
  type: ClusterIP
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: postgresql
    release: "airflow"
    role: master
---
# Source: airflow/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-redis-headless
  labels:
    app: redis
    chart: redis-10.5.7
    release: airflow
    heritage: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: redis
    port: 6379
    targetPort: redis
  selector:
    app: redis
    release: airflow
---
# Source: airflow/charts/redis/templates/redis-master-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-redis-master
  labels:
    app: redis
    chart: redis-10.5.7
    release: airflow
    heritage: Helm
spec:
  type: ClusterIP
  ports:
  - name: redis
    port: 6379
    targetPort: redis
  selector:
    app: redis
    release: airflow
    role: master
---
# Source: airflow/templates/webserver/webserver-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-web
  labels:
    app: airflow
    component: web
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: web
    release: airflow
  sessionAffinity: None
  ports:
    - name: web
      protocol: TCP
      port: 8080
      targetPort: 8080
---
# Source: airflow/templates/worker/worker-service.yaml
apiVersion: v1
## this Service gives stable DNS entries for workers, used by webserver for logs
kind: Service
metadata:
  name: airflow-worker
  labels:
    app: airflow
    component: worker
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
spec:
  ports:
    - name: worker
      protocol: TCP
      port: 8793
  clusterIP: None
  selector:
    app: airflow
    component: worker
    release: airflow
---
# Source: airflow/templates/db-migrations/db-migrations-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-db-migrations
  labels:
    app: airflow
    component: db-migrations
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
spec:
  replicas: 1
  strategy:
    ## only 1 replica should run at a time
    type: Recreate
  selector:
    matchLabels:
      app: airflow
      component: db-migrations
      release: airflow
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: b5408f467bab9070f96c260c930f6c013168c09cf0b0616b9c0822f18dead906
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: db-migrations
        release: airflow
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 1000690000
      serviceAccountName: airflow
      initContainers:        
        - name: check-db  
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      containers:
        - name: db-migrations          
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "python"
            - "-u"
            - "/mnt/scripts/db_migrations.py"
          volumeMounts:            
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
            - name: scripts
              mountPath: /mnt/scripts
              readOnly: true
      volumes:        
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-pvc
        - name: scripts
          secret:
            secretName: airflow-db-migrations
---
# Source: airflow/templates/scheduler/scheduler-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-scheduler
  labels:
    app: airflow
    component: scheduler
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple schedulers can run concurrently (Airflow 2.0)
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: scheduler
      release: airflow
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: b5408f467bab9070f96c260c930f6c013168c09cf0b0616b9c0822f18dead906
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: scheduler
        release: airflow
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 1000690000
      serviceAccountName: airflow
      initContainers:        
        - name: check-db  
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath:         
        - name: wait-for-db-migrations  
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      containers:
        - name: airflow-scheduler          
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          resources:
            limits:
              cpu: 2
              memory: 2Gi
            requests:
              cpu: 2
              memory: 2Gi
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow scheduler -n -1"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 30
            failureThreshold: 5
            timeoutSeconds: 60
            exec:
              command:                
                - "/usr/bin/dumb-init"
                - "--"
                - "/entrypoint"
                - "python"
                - "-Wignore"
                - "-c"
                - |
                  import os
                  import sys

                  # suppress logs triggered from importing airflow packages
                  os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                  # shared imports
                  try:
                      from airflow.jobs.job import Job
                  except ImportError:
                      # `BaseJob` was renamed to `Job` in airflow 2.6.0
                      from airflow.jobs.base_job import BaseJob as Job
                  from airflow.utils.db import create_session
                  from airflow.utils.net import get_hostname

                  # heartbeat check imports
                  try:
                      from airflow.jobs.scheduler_job_runner import SchedulerJobRunner
                  except ImportError:
                      # `SchedulerJob` is wrapped by `SchedulerJobRunner` since airflow 2.6.0
                      from airflow.jobs.scheduler_job import SchedulerJob as SchedulerJobRunner

                  with create_session() as session:
                      ########################
                      # heartbeat check
                      ########################
                      # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                      hostname = get_hostname()
                      scheduler_job = session \
                          .query(Job) \
                          .filter_by(job_type=SchedulerJobRunner.job_type) \
                          .filter_by(hostname=hostname) \
                          .order_by(Job.latest_heartbeat.desc()) \
                          .limit(1) \
                          .first()
                      if (scheduler_job is not None) and scheduler_job.is_alive():
                          pass
                      else:
                          sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
          volumeMounts:            
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      volumes:        
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-pvc
---
# Source: airflow/templates/sync/sync-users-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-sync-users
  labels:
    app: airflow
    component: sync-users
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
spec:
  replicas: 1
  strategy:
    ## only 1 replica should run at a time
    type: Recreate
  selector:
    matchLabels:
      app: airflow
      component: sync-users
      release: airflow
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: b5408f467bab9070f96c260c930f6c013168c09cf0b0616b9c0822f18dead906
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        checksum/sync-users-script: 9733d90da42fbc9e6b1e60df36e22c87aa7c1114a86eb96bb2eae03de9a3b046
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: sync-users
        release: airflow
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 1000690000
      serviceAccountName: airflow
      initContainers:        
        - name: check-db  
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath:         
        - name: wait-for-db-migrations  
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      containers:
        - name: sync-airflow-users          
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "python"
            - "-u"
            - "/mnt/scripts/sync_users.py"
          volumeMounts:            
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
            - name: scripts
              mountPath: /mnt/scripts
              readOnly: true
      volumes:        
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-pvc
        - name: scripts
          secret:
            secretName: airflow-sync-users
---
# Source: airflow/templates/triggerer/triggerer-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-triggerer
  labels:
    app: airflow
    component: triggerer
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple triggerer pods can safely run concurrently
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: triggerer
      release: airflow
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: b5408f467bab9070f96c260c930f6c013168c09cf0b0616b9c0822f18dead906
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: triggerer
        release: airflow
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      serviceAccountName: airflow
      securityContext:
        fsGroup: 1000690000
      initContainers:        
        - name: check-db  
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath:         
        - name: wait-for-db-migrations  
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      containers:
        - name: airflow-triggerer          
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow triggerer"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 30
            timeoutSeconds: 60
            failureThreshold: 5
            exec:
              command:                
                - "/usr/bin/dumb-init"
                - "--"
                - "/entrypoint"
                - "python"
                - "-Wignore"
                - "-c"
                - |
                  import os
                  import sys

                  # suppress logs triggered from importing airflow packages
                  os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                  # shared imports
                  try:
                      from airflow.jobs.job import Job
                  except ImportError:
                      # `BaseJob` was renamed to `Job` in airflow 2.6.0
                      from airflow.jobs.base_job import BaseJob as Job
                  from airflow.utils.db import create_session
                  from airflow.utils.net import get_hostname

                  # heartbeat check imports
                  try:
                      from airflow.jobs.triggerer_job_runner import TriggererJobRunner
                  except ImportError:
                      # `TriggererJob` is wrapped by `TriggererJobRunner` since airflow 2.6.0
                      from airflow.jobs.triggerer_job import TriggererJob as TriggererJobRunner

                  with create_session() as session:
                      # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                      hostname = get_hostname()
                      triggerer_job = session \
                          .query(Job) \
                          .filter_by(job_type=TriggererJobRunner.job_type) \
                          .filter_by(hostname=hostname) \
                          .order_by(Job.latest_heartbeat.desc()) \
                          .limit(1) \
                          .first()
                      if (triggerer_job is not None) and triggerer_job.is_alive():
                          pass
                      else:
                          sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
          volumeMounts:            
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      volumes:        
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-pvc
---
# Source: airflow/templates/webserver/webserver-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-web
  labels:
    app: airflow
    component: web
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple web pods can safely run concurrently
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: web
      release: airflow
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: b5408f467bab9070f96c260c930f6c013168c09cf0b0616b9c0822f18dead906
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        checksum/config-webserver-config: 08f3293f0aaa3978bcbcae61eee58760bcba270cf691c5ff9c788d678ba8d579
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: web
        release: airflow
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      serviceAccountName: airflow
      securityContext:
        fsGroup: 1000690000
      initContainers:        
        - name: check-db  
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath:         
        - name: wait-for-db-migrations  
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      containers:
        - name: airflow-web          
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          resources:
            {}
          ports:
            - name: web
              containerPort: 8080
              protocol: TCP
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow webserver"
          livenessProbe:
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            httpGet:
              scheme: HTTP
              path: /health
              port: web
          readinessProbe:
            initialDelaySeconds: 45
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            httpGet:
              scheme: HTTP
              path: /health
              port: web
          volumeMounts:            
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
            - name: webserver-config
              mountPath: /opt/airflow/webserver_config.py
              subPath: webserver_config.py
              readOnly: true
      volumes:        
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-pvc
        - name: webserver-config
          secret:
            secretName: airflow-webserver-config
            defaultMode: 0644
---
# Source: airflow/charts/postgresql/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: airflow-postgresql
  labels:
    app: postgresql
    chart: postgresql-8.6.4
    release: "airflow"
    heritage: "Helm"
spec:
  serviceName: airflow-postgresql-headless
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: postgresql
      release: "airflow"
      role: master
  template:
    metadata:
      name: airflow-postgresql
      labels:
        app: postgresql
        chart: postgresql-8.6.4
        release: "airflow"
        heritage: "Helm"
        role: master
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    spec:      
      securityContext:
        fsGroup: 1.00069e+09
      initContainers:
        # - name: do-something
        #   image: busybox
        #   command: ['do', 'something']
        
      containers:
        - name: airflow-postgresql
          image: ghcr.io/airflow-helm/postgresql-bitnami:11.16-patch.0
          imagePullPolicy: "IfNotPresent"
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            runAsUser: 0
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            - name: POSTGRES_USER
              value: "postgres"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: POSTGRES_DB
              value: "airflow"
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -d "airflow" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "postgres" -d "airflow" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
              subPath: 
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: airflow/charts/redis/templates/redis-master-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: airflow-redis-master
  labels:
    app: redis
    chart: redis-10.5.7
    release: airflow
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: redis
      release: airflow
      role: master
  serviceName: airflow-redis-headless
  template:
    metadata:
      labels:
        app: redis
        chart: redis-10.5.7
        release: airflow
        role: master
      annotations:
        checksum/health: a5073935c8eb985cf8f3128ba7abbc4121cef628a9a1b0924c95cf97d33323bf
        checksum/configmap: 2b82c78fd9186045e6e2b44cfbb38460310697cf2f2f175c9d8618dd4d42e1ca
        checksum/secret: 2b847ea7129d7018fe67bde0a99fe18debd3dc92ce000033e05a221b924aa82c
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    spec:      
      securityContext:
        fsGroup: 1.00069e+09
      serviceAccountName: "default"
      containers:
      - name: airflow-redis
        image: "docker.io/bitnami/redis:5.0.14-debian-10-r173"
        imagePullPolicy: "IfNotPresent"
        securityContext:
          runAsUser: 1000690000
        command:
        - /bin/bash
        - -c
        - |
          if [[ -n $REDIS_PASSWORD_FILE ]]; then
            password_aux=`cat ${REDIS_PASSWORD_FILE}`
            export REDIS_PASSWORD=$password_aux
          fi
          if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
            cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
          fi
          if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
            cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
          fi
          ARGS=("--port" "${REDIS_PORT}")
          ARGS+=("--requirepass" "${REDIS_PASSWORD}")
          ARGS+=("--masterauth" "${REDIS_PASSWORD}")
          ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
          ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
          /run.sh ${ARGS[@]}
        env:
        - name: REDIS_REPLICATION_MODE
          value: master
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: airflow-redis
              key: redis-password
        - name: REDIS_PORT
          value: "6379"
        ports:
        - name: redis
          containerPort: 6379
        livenessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
          exec:
            command:
            - sh
            - -c
            - /health/ping_liveness_local.sh 5
        readinessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 1
          successThreshold: 1
          failureThreshold: 5
          exec:
            command:
            - sh
            - -c
            - /health/ping_readiness_local.sh 5
        resources:
          {}
        volumeMounts:
        - name: health
          mountPath: /health
        - name: redis-data
          mountPath: /data
          subPath: 
        - name: config
          mountPath: /opt/bitnami/redis/mounted-etc
        - name: redis-tmp-conf
          mountPath: /opt/bitnami/redis/etc/
      volumes:
      - name: health
        configMap:
          name: airflow-redis-health
          defaultMode: 0755
      - name: config
        configMap:
          name: airflow-redis
      - name: "redis-data"
        emptyDir: {}
      - name: redis-tmp-conf
        emptyDir: {}
  updateStrategy:
    type: RollingUpdate
---
# Source: airflow/templates/worker/worker-statefulset.yaml
apiVersion: apps/v1
## StatefulSet gives workers consistent DNS names, allowing webserver access to log files
kind: StatefulSet
metadata:
  name: airflow-worker
  labels:
    app: airflow
    component: worker
    chart: airflow-8.8.0
    release: airflow
    heritage: Helm
spec:
  serviceName: "airflow-worker"
  replicas: 2
  updateStrategy:
    type: RollingUpdate
  ## we do not need to guarantee the order in which workers are scaled
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: airflow
      component: worker
      release: airflow
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: b5408f467bab9070f96c260c930f6c013168c09cf0b0616b9c0822f18dead906
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: worker
        release: airflow
    spec:
      restartPolicy: Always
      terminationGracePeriodSeconds: 60
      serviceAccountName: airflow
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 1000690000
      initContainers:        
        - name: check-db  
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath:         
        - name: wait-for-db-migrations  
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          envFrom:    
            - secretRef:
                name: airflow-config-envs
          env:    
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      containers:
        - name: airflow-worker          
          image: apache/airflow:2.6.3-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          resources:
            limits:
              cpu: 1
              memory: 3Gi
            requests:
              cpu: 1
              memory: 1Gi
          envFrom:            
            - secretRef:
                name: airflow-config-envs
          env:            
            - name: DATABASE_USER
              value: "postgres"
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgresql
                  key: postgresql-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
            # have dumb-init only send signals to direct child process (needed for celery workers to warm shutdown)
            - name: DUMB_INIT_SETSID
              value: "0"
          livenessProbe:
            initialDelaySeconds: 10
            timeoutSeconds: 60
            failureThreshold: 5
            periodSeconds: 30
            exec:
              command:                
                - "/usr/bin/dumb-init"
                - "--"
                - "/entrypoint"
                - "python"
                - "-Wignore"
                - "-c"
                - |
                  import os
                  import sys
                  import subprocess
                  from celery import Celery
                  from celery.app.control import Inspect
                  from typing import List

                  def run_command(cmd: List[str]) -> str:
                      process = subprocess.Popen(cmd, stdout=subprocess.PIPE)
                      output, error = process.communicate()
                      if error is not None:
                          raise Exception(error)
                      else:
                          return output.decode(encoding="utf-8")

                  broker_url = run_command(["bash", "-c", "eval $AIRFLOW__CELERY__BROKER_URL_CMD"])
                  local_celery_host = f"celery@{os.environ['HOSTNAME']}"
                  app = Celery(broker=broker_url)

                  # ping the local celery worker to see if it's ok
                  i = Inspect(app=app, destination=[local_celery_host], timeout=5.0)
                  ping_responses = i.ping()
                  if local_celery_host not in ping_responses:
                      sys.exit(f"celery worker '{local_celery_host}' did not respond to ping")
          ports:
            - name: wlog
              containerPort: 8793
              protocol: TCP
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow celery worker"
          volumeMounts:            
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      volumes:        
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-pvc
